{
    "componentChunkName": "component---gatsby-theme-academic-src-pages-bookshelf-index-jsx",
    "path": "/bookshelf/",
    "result": {"data":{"allNotionBook":{"nodes":[{"notionId":"29c7627b-f2eb-8041-bdd6-db6ed65e7ede","title":"Scaling Latent Reasoning via Looped Language Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2510.25741","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"2957627b-f2eb-80ba-a2b4-c3faf682a951","title":"How to Train Your Energy-Based Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2101.03288","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"2937627b-f2eb-80fe-b67f-e834f90a480f","title":"Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"2937627b-f2eb-800f-bd38-c7436e884bc4","title":"Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2504.10612v1","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"2937627b-f2eb-804e-99e0-ee33188387c5","title":"Deep Equilibrium Approaches to Diffusion Models","author":null,"medium":"Research Paper","enjoyment":6,"importance":5,"link":"https://arxiv.org/abs/2210.12867#:~:text=perspective%2C%20that%20of%20a%20,used%20to%20reduce%20the%20sampling","notes":"The DEQ version of DDIM represents all diffusion states $x_{0:T}$ simultaneously within a single DEQ state. It claims that DEQ formulation is great for Model Inversion task as it only need to do one back-prop at the fix point","twoCents":"Provided me with a more detailed sense of how DDPM and DDIM can be related to DEQ","keywords":["Diffusion","DEQ"],"dateSaved":null,"display":true},{"notionId":"2927627b-f2eb-804b-8a74-c4718a28eda4","title":"The Mathematical Building Blocks of Diffusion Generative Models","author":null,"medium":"Blog","enjoyment":9,"importance":9,"link":"https://tonghe-zhang.github.io/blogs/fokker-planck-equation.html","notes":"Going over Divergence, Score Function, Hessian and Laplacian; Proved necessary condition of Fokker-Planck Equation; connected FP equation to ODE, forward / reverse diffusion","twoCents":"Decently Well, put all the preliminaries at the beginning, serving as a good way for readers to go over necessary knowledge. The proof of FP equation is great, and the reverse diffusion procedure derived based on it is also great. Built and intuition of how the coefficient of drift term in the backward procedure is derived. Not being too verbose while also catching all necessary backgrounds, a good example of what Karpathy described as “learn when you needed”, a good balance of DFS and BFS.","keywords":["Diffusion","theory","math"],"dateSaved":null,"display":true},{"notionId":"2927627b-f2eb-8064-af0d-e9ba45bbd1e8","title":"初探MuP：超参数的跨模型尺度迁移规律","author":null,"medium":"Blog","enjoyment":null,"importance":null,"link":"https://kexue.fm/archives/10770","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"2887627b-f2eb-801a-a136-edeafc0bf14d","title":"Scaling up Test-Time Compute with Latent Reasoning","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2502.05171","notes":null,"twoCents":"","keywords":["DEQ","LLM"],"dateSaved":null,"display":false},{"notionId":"2877627b-f2eb-80e0-a008-f2ba654db795","title":"DataComp-LM: In search of the next generation of training sets for language models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2406.11794","notes":null,"twoCents":"","keywords":["Data","LLM"],"dateSaved":null,"display":false},{"notionId":"2877627b-f2eb-800e-acb4-ebafb11e55cd","title":"EvoLM: In Search of Lost Language Model Training Dynamics","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2506.16029","notes":null,"twoCents":"","keywords":["LLM","Data"],"dateSaved":null,"display":false},{"notionId":"2757627b-f2eb-80a6-90e5-d224caaffcb9","title":"Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2509.03646","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"2747627b-f2eb-808c-98d1-c79541eb717a","title":"Mixure of Recursions","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2507.10524","notes":null,"twoCents":"","keywords":["DEQ"],"dateSaved":null,"display":false},{"notionId":"2717627b-f2eb-80b1-ad28-ce47e3557331","title":"Understanding Transformers through the Lens of Pavlovian Conditioning","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2508.08289","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"26f7627b-f2eb-8086-8bee-ec5dddf1a5d7","title":"Path Independent Equilibrium Models Can Better Exploit Test-Time Computation","author":null,"medium":null,"enjoyment":null,"importance":null,"link":null,"notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"26d7627b-f2eb-805f-a1f7-e249b9f3680f","title":"Deep Equilibrium Optical Flow Estimation","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2204.08442","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"26d7627b-f2eb-80e0-a76e-eee7ab6f1204","title":"Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2106.04537v2","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"26d7627b-f2eb-8058-a717-c7007255bfce","title":"Deep Equilibrium Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":null,"notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"26d7627b-f2eb-80fd-b4e7-f3e874f63267","title":"Hierarchical Reasoning Model","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2506.21734","notes":null,"twoCents":"","keywords":["DEQ"],"dateSaved":null,"display":false},{"notionId":"26d7627b-f2eb-8038-82c0-e0158fa3b260","title":"On Training Implicit Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2111.05177","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"26c7627b-f2eb-800f-a897-f9e52613fad0","title":"Implicit Language Models are RNNs: Balancing Parallelization and Expressivity","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2502.07827","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"26c7627b-f2eb-8015-996c-ff37d790d9e6","title":"Is Attention Better Than Matrix Decomposition?","author":null,"medium":null,"enjoyment":8,"importance":6,"link":"http://arxiv.org/abs/2109.04553","notes":null,"twoCents":"","keywords":["DEQ"],"dateSaved":null,"display":false},{"notionId":"26c7627b-f2eb-808f-acb9-fbff7a940c9a","title":"Mixture-of-Depths: Dynamically allocating\ncompute in transformer-based language\nmodels","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/pdf/2404.02258","notes":null,"twoCents":"","keywords":["DEQ"],"dateSaved":null,"display":false},{"notionId":"26c7627b-f2eb-8045-bed8-e1890b306e2c","title":"Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2502.13842","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"2387627b-f2eb-8023-85d9-f0e0b0a2e440","title":"An Empirical Study of Mamba-based Language Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/pdf/2406.07887","notes":null,"twoCents":"","keywords":["SSM"],"dateSaved":null,"display":false},{"notionId":"22d7627b-f2eb-8078-8387-d821f8200185","title":"Geometry Forcing","author":null,"medium":null,"enjoyment":null,"importance":null,"link":" https://arxiv.org/pdf/2507.07982","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"1d17627b-f2eb-808a-b3a9-f296a1c2b506","title":"GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/pdf/2411.08033","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"1c57627b-f2eb-80bf-ac58-d6dbfb9c46a3","title":"The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2402.04347","notes":null,"twoCents":"","keywords":["Linear-attention"],"dateSaved":null,"display":false},{"notionId":"1c17627b-f2eb-80f5-a127-ebc4152f8ace","title":"CFG-Zero⋆: Improved Classifier-Free Guidance for Flow Matching Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/pdf/2503.18886","notes":null,"twoCents":"","keywords":[],"dateSaved":null,"display":false},{"notionId":"1c17627b-f2eb-8073-b499-fc58ed781b92","title":"Position: Interactive Generative Video as Next-Generation Game Engine","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/pdf/2503.17359","notes":null,"twoCents":"","keywords":["position-paper"],"dateSaved":null,"display":false},{"notionId":"1bf7627b-f2eb-8091-8e6e-e245239cf203","title":"SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2410.10629","notes":null,"twoCents":"","keywords":["Model","Linear-attention"],"dateSaved":null,"display":false},{"notionId":"1bf7627b-f2eb-80d9-a01c-fc6b56087c5e","title":"MoM: Linear Sequence Modeling with Mixture-of-Memories","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2502.13685","notes":null,"twoCents":"This architecture is compatible with history guidance diffusion forcing","keywords":["spatial memory","Model"],"dateSaved":null,"display":false},{"notionId":"1bc7627b-f2eb-8050-b231-feb1441206df","title":"Learning 3D Persistent Embodied World Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":null,"notes":null,"twoCents":"","keywords":["embodied task","3D-Memory"],"dateSaved":null,"display":false},{"notionId":"1bc7627b-f2eb-80c3-9c3c-df3d7bb13c4b","title":"3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2411.17735","notes":null,"twoCents":"","keywords":["embodied task","3D-Memory","important"],"dateSaved":null,"display":false},{"notionId":"1bc7627b-f2eb-80ee-964b-eea5efe41577","title":"MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://github.com/facebookresearch/MeMViT/tree/main/memvit","notes":null,"twoCents":"The architecture here is inspiring","keywords":["Model"],"dateSaved":null,"display":false},{"notionId":"1bc7627b-f2eb-8097-9361-ecd1a3ef70eb","title":"InfinityDrive: Breaking Time Limits in Driving World Models","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2412.01522","notes":null,"twoCents":"","keywords":["auto driving","Model"],"dateSaved":null,"display":false},{"notionId":"1ba7627b-f2eb-80e1-8f09-fe80c879f91d","title":"Compositional Generative Modeling: A Single Model is Not All You Need","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/pdf/2402.01103","notes":null,"twoCents":"","keywords":["CFG"],"dateSaved":null,"display":false},{"notionId":"1ba7627b-f2eb-800e-a96f-d4ed2455b1ae","title":"History Guided Diffusion Forcing Transformer","author":null,"medium":null,"enjoyment":null,"importance":null,"link":null,"notes":null,"twoCents":"","keywords":["Diffusion Forcing","CFG","important"],"dateSaved":null,"display":false},{"notionId":"1b67627b-f2eb-8008-8d69-e2ab021b2b92","title":"DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2405.14224","notes":null,"twoCents":"","keywords":["Model","Mamba"],"dateSaved":null,"display":false},{"notionId":"1b67627b-f2eb-80bd-849b-c3412cd9f74f","title":"FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling   ","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"https://arxiv.org/abs/2310.15169","notes":null,"twoCents":"Help understanding the noise scheduling, loss reweighting in diffusion training","keywords":["Noise Scheduling","algorithms"],"dateSaved":null,"display":false},{"notionId":"1b67627b-f2eb-8053-b8bb-fad4d999d795","title":"Evaluating Long-Term Memory in 3D Mazes","author":null,"medium":null,"enjoyment":null,"importance":null,"link":"http://arxiv.org/abs/2210.13383","notes":null,"twoCents":"","keywords":["Benchmark","RL"],"dateSaved":null,"display":false}]}},"pageContext":{"slug":"/bookshelf/","langKey":"en"}},
    "staticQueryHashes": ["1552981879","1858131618","4097791827"]}