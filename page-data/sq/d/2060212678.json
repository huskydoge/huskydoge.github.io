{"data":{"allTag":{"edges":[{"node":{"name":"World Model","color":"magenta","path":"/tags/World Model"}},{"node":{"name":"Video Generation","color":"green","path":"/tags/Video Generation"}},{"node":{"name":"Long-term Consistency","color":"red","path":"/tags/Long-term Consistency"}},{"node":{"name":"Diffusion","color":"magenta","path":"/tags/Diffusion"}},{"node":{"name":"LLM","color":"red","path":"/tags/LLM"}},{"node":{"name":"Efficiency","color":"magenta","path":"/tags/Efficiency"}},{"node":{"name":"Equivariant Models","color":"orange","path":"/tags/Equivariant Models"}},{"node":{"name":"LLM Agent","color":"lime","path":"/tags/LLM Agent"}},{"node":{"name":"Benchmark","color":"magenta","path":"/tags/Benchmark"}},{"node":{"name":"Data-centric AI","color":"magenta","path":"/tags/Data-centric AI"}},{"node":{"name":"writing","color":"geekblue","path":"/tags/writing"}},{"node":{"name":"research","color":"blue","path":"/tags/research"}},{"node":{"name":"skills","color":"red","path":"/tags/skills"}},{"node":{"name":"paper","color":"red","path":"/tags/paper"}},{"node":{"name":"gatsby","color":"cyan","path":"/tags/gatsby"}},{"node":{"name":"blog","color":"cyan","path":"/tags/blog"}},{"node":{"name":"test1","color":"blue","path":"/tags/test1"}},{"node":{"name":"test2","color":"blue","path":"/tags/test2"}},{"node":{"name":"test3","color":"blue","path":"/tags/test3"}},{"node":{"name":"influence function","color":"blue","path":"/tags/influence function"}},{"node":{"name":"Data Attribution","color":"orange","path":"/tags/Data Attribution"}},{"node":{"name":"Survey","color":"green","path":"/tags/Survey"}},{"node":{"name":"GenAI","color":"gold","path":"/tags/GenAI"}},{"node":{"name":"Game Generation","color":"gold","path":"/tags/Game Generation"}},{"node":{"name":"Adversarial Perturbations","color":"volcano","path":"/tags/Adversarial Perturbations"}},{"node":{"name":"AI Safety","color":"volcano","path":"/tags/AI Safety"}},{"node":{"name":"Image to Video","color":"lime","path":"/tags/Image to Video"}},{"node":{"name":"AI Interpretability","color":"volcano","path":"/tags/AI Interpretability"}},{"node":{"name":"ML","color":"lime","path":"/tags/ML"}},{"node":{"name":"3D-Reconstruction","color":"red","path":"/tags/3D-Reconstruction"}},{"node":{"name":"CV","color":"orange","path":"/tags/CV"}},{"node":{"name":"Coq","color":"orange","path":"/tags/Coq"}},{"node":{"name":"FormalVerification","color":"gold","path":"/tags/FormalVerification"}},{"node":{"name":"AGI(Image)","color":"volcano","path":"/tags/AGI(Image)"}},{"node":{"name":"Evaluation","color":"gold","path":"/tags/Evaluation"}},{"node":{"name":"Model-Arch","color":"lime","path":"/tags/Model-Arch"}},{"node":{"name":"React","color":"green","path":"/tags/React"}},{"node":{"name":"MongoDB","color":"green","path":"/tags/MongoDB"}},{"node":{"name":"RL","color":"green","path":"/tags/RL"}},{"node":{"name":"DDQN","color":"orange","path":"/tags/DDQN"}},{"node":{"name":"MCTS","color":"lime","path":"/tags/MCTS"}},{"node":{"name":"C++","color":"volcano","path":"/tags/C++"}},{"node":{"name":"Game Design","color":"gold","path":"/tags/Game Design"}},{"node":{"name":"Vue.js","color":"cyan","path":"/tags/Vue.js"}},{"node":{"name":"echarts","color":"cyan","path":"/tags/echarts"}},{"node":{"name":"element-ui","color":"cyan","path":"/tags/element-ui"}}]},"allMdx":{"edges":[{"node":{"frontmatter":{"cover":{"publicURL":"/static/7e21123f407fca3316b5629d6441314d/pan.gif"},"date":"2025-11-13","venue":"","authors":["Jiannan Xiang","Yi Gu","Zihan Liu","Zeyu Feng","Qiyue Gao","Yiyan Hu","**Benhao Huang**","Guangyi Liu","Yichi Yang","Kun Zhou","Davit Abrahamyan","Arif Ahmad","Ganesh Bannur","Junrong Chen","Kimi Chen","Mingkai Deng","Ruobing Han","Xinqi Huang","Haoqiang Kang","Zheqi Li","Enze Ma","Hector Ren","Yashowardhan Shinde","Rohan Shingre","Ramsundar Tanikella","Kaiming Tao","Dequan Yang","Xinle Yu","Cong Zeng","Binglin Zhou","Hector Liu","Zhiting Hu","Eric P. Xing"],"path":"research/2025/pan","title":"PAN: A World Model for General, Interactable, and Long-Horizon World Simulation","tags":["World Model","Video Generation","Long-term Consistency"],"excerpt":"PAN brings imagination to life â€” fusing language, action, and vision to simulate the world's evolution with stunning realism and consistency.","selected":true,"priority":-1,"highlight":true,"links":[{"name":"technical report","url":"https://arxiv.org/abs/2511.09057"},{"name":"Blog","url":"https://panworld.ai/"}]},"fileAbsolutePath":"/home/runner/work/huskydoge.github.io/huskydoge.github.io/example/content/research/2025/pan/index.md"}},{"node":{"frontmatter":{"cover":{"publicURL":"/static/1ff8e96d5f97dd47e6d838f3442afae4/flowm_preview.gif"},"date":"2025-09-23","venue":"[NeurReps Workshop](https://www.neurreps.org/), [SpaVLE Workshop](https://space-in-vision-language-embodied-ai.github.io/) @ NeurIPS 2025","authors":["[Hansen Lillemark*](https://hlillemark.github.io/)","**Benhao Huang***","[Fangneng Zhan](https://fnzhan.com/)","[Yilun Du](https://yilundu.github.io/)","[T. Anderson Keller](https://akandykeller.github.io/about/)"],"path":"research/2025/flowm","title":"Flow Equivariant World Models: Structured Dynamics Outside the Field of View","tags":["World Model","Equivariant Models","Long-term Consistency"],"excerpt":"On World Modeling the partially observable dynamics in the environment.","selected":true,"priority":0,"highlight":true,"links":[{"name":"paper","url":"https://arxiv.org/abs/2601.01075"},{"name":"code","url":"https://github.com/hlillemark/flowm"},{"name":"website","url":"https://flowequivariantworldmodels.github.io/"}]},"fileAbsolutePath":"/home/runner/work/huskydoge.github.io/huskydoge.github.io/example/content/research/2025/flowm/index.md"}},{"node":{"frontmatter":{"cover":{"publicURL":"/static/39871772631160338fa8b4f16f5ab1c1/preview.png"},"date":"2025-05-01","venue":"[KDD-2025 DB Track](https://kdd2025.kdd.org/call-for-datasets-and-benchmarks-track-papers/) **(Oral)**, [ICML-2025 Data World](https://dataworldicml2025.github.io/)","authors":["**Benhao Huang**","[Yingzhuo Yu](https://www.linkedin.com/in/yingzhuo-yu-302334232/)","[Jin Huang](https://jn-huang.github.io/)","[Xingjian Zhang](https://sites.google.com/umich.edu/xingjian-zhang)","[Jiaqi W. Ma](https://jiaqima.github.io/)"],"path":"research/2025/dca","title":"DCA-Bench: A Benchmark for Dataset Curation Agents","tags":["LLM Agent","Benchmark","Data-centric AI"],"excerpt":"A benchmark exploring the performance of LLM Agents on detecting issues in datasets hosted on popular platforms.","selected":true,"priority":1,"highlight":true,"links":[{"name":"paper","url":"https://arxiv.org/abs/2406.07275"},{"name":"Github","url":"https://github.com/TRAIS-Lab/dca-bench"},{"name":"HuggingFace","url":"https://huggingface.co/datasets/trais-lab/DCA-Bench"},{"name":"slides","url":"/research/2025/dca/oral_slides.pdf"},{"name":"poster","url":"/research/2025/dca/kdd_poster.pdf"}]},"fileAbsolutePath":"/home/runner/work/huskydoge.github.io/huskydoge.github.io/example/content/research/2025/dca/index.md"}}]}}}