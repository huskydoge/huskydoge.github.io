{
    "componentChunkName": "component---gatsby-theme-academic-src-templates-post-post-jsx",
    "path": "/research/2025/wm-bench",
    "result": {"data":{"mdx":{"timeToRead":1,"tableOfContents":{},"frontmatter":{"cover":{"childImageSharp":{"fluid":{"tracedSVG":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='320'%20height='180'%20viewBox='0%200%20320%20180'%20preserveAspectRatio='none'%3e%3cpath%20d='M252%202l-1%206v5h5c4%200%205-1%207-2V9c-2%201-2%200-2-3s0-3-4-3c-2%200-3%200-2-1h-3m39%2016c-6%202-7%204-5%206l3%202-1-2c-1-2-1-2%201-4%203-3%204-2%203%200-1%203-1%203%201%205%201%202%201%202-1%202h-1l5%201%205-1h-1c-2%200-2%200-1-2%202-2%202-2%201-5-1-2%200-3%202-1l1%201v-2c-2-2-9-2-12%200m-106%201c-1%202%200%203%202%204%201%201%201%201-1%201-3%200-2%202%201%202l3-2c0-2%200-2%201%200%201%203%208%203%208%200l1-2%201%201c0%202%200%203%202%203v-4l1%202c1%203%205%203%205%200s-1-3-4-3l-2-1h-2l-3%201h-2v2h-3c1-3-1-5-2-3s-2%202-4%201c-1-1-1-1%201-1l2-1h-5m-37%206l-3%201c-2%200-3%200-3%203v4h-17c-15%200-16%200-16%202h1a289%20289%200%200157%200c2-1%202-1%200-3h-3l-10%201-11-1%202-1%202-2%201-2v2l2%202c2%200%202-1%202-3v-2l-1%202c0%202-1%202-1%201v-2c0-2-1-3-2-2m-68%204v1l1%202c0%202-2%203-3%200h-1l2%203c2%201%202%201%200%203l-1%205c1%203%200%203-1%203-4%200-5%204-3%206%203%205%206%203%206-4-1-2%200-2%202-2s3%200%202%202l1%204c1%204%205%204%206-1%201-4%201-4-2-5h-3v-2l-1-5c-2-2-2-3%200-4v-2l-1-1h3l-5-4-2%201m115%202l-2%203v3c-3%202-2%205%201%205%202%201%201%201-2%201l-4%201-2%204c-3%202-1%206%202%206%205%200%206-4%203-7l-2-2h8c8%200%208%200%206%202-4%202-2%208%203%207%203-1%204-4%201-8-2-2-2-2%200-4%204-4-1-11-6-7-1%201-1%201-1-1%200-4-3-6-5-3m62%202c1%201%201%201-1%201-3-1-5%200-3%202%201%200%201%201-1%201v1l4-1c2-2%203-2%206%200%206%204%202%2012-6%2010h-5c-1%202-1%202%203%202%208%201%209%200%2012-3%202-2%202-3%202-5l-1-5c0-3-2-4-3-2-1%201-1%201-2-1%200-1-1-2-3-2s-2%200-2%202m31%2018l-1%204v2l1%202%202-2c2-2%202-2%203%200l3%202v1l-5%201-4%201h12c-2%201-2%201%201%201h3l-9%202-7%201h7c7-1%209-1%2010-3%202-2%202-2%201-3h-5c-2-1-2-1%200-1%201-1%202-1%201-3%200-2%200-2%201-1%202%200%203-3%201-4h-1l-3%201c-2%200-3%200-2%202s-2%201-3-2l-2-2-1%202c0%204-2%204-1%200l-1-4c-1-1-1%200-1%203M139%2077l-1%203c-3%200-3%203-1%203s2%200%200%201c-4%201-5%203-3%203%203%200%200%205-7%2012-5%204-5%205-7%204-1-1-2-2-4-1-2%200-3%200-4-2-2-3-8-3-11%201l-1%203-2-3c-3-4-3-4-6%203-3%206-3%206-1%206l1-1%203-1c2%200%203%200%203%202s0%202%202%201l1-3c-1-3%200-5%201-5l1%204c0%203%200%203%204%203l5-1%202-1c1%201%201%202-1%203h1l7-1c4%200%205%200%205-3%200-4%202-4%202%200%200%202%201%203%202%203%202%200%202-8-1-8-1-1%203-8%209-15l1-2h3l-1-4-1-3-1-3v2m-39%205l1%203%201%201%204%208v-1c-1-1%200-3%201-5l2-4%201%203c0%202%202%203%202%203%202%200%205-7%205-10s-2-2-4%202l-1%204-1-4c-1-5-3-5-4-1l-2%203-2-3c-1-4-3-4-3%201m19%202c0%205%200%206%202%206l1-2c0-2%200-2%202%200h3c1-2%201-2%201%200l2%202%201-6c0-7-1-8-4-3l-2%203-2-3c-3-5-4-4-4%203m-95%203c-5%203-8%209-8%2015%202%2014%2022%2018%2028%205%207-13-8-27-20-20m8%201c2%202%202%209%201%209s-2%201-1%202c0%201%200%202-2%202l-4%202-2%203h13l3%201c2%202%202%202%203-1s0-6-3-7-3-1-3-4%200-4-2-6-4-3-3-1m108%2012c0%202-1%203-2%201-1-1-6%203-6%205s2%204%205%204l2%202c1%201%201%201%201-1l2-2c1%200%202%200%202-2%200-4%202-4%202%200%200%202%201%203%202%203%202%200%202-7-1-8h-3l-1-2c1-2-2-3-3%200m87%2021l-5%2013-1%2023v23h13v-71l-7%2012m49%204c-1%200-2%201-2%203l-3%202-2%201-2%203v5l2%202%202%201c0%202%207%202%207%200l2-1%202-3%201-2c3-2%203-4%201-9l-2-2h-2l-1-1-3%201m-98%2023l1%202v3c1%202%205%200%205-2l1%201%204%202c4%200%204%200%204-3v-2l1%202c1%202%202%203%205%203s4-1%204-3-1-3-3-3l-4-1h-1l-4%201-2%201-1%203-1-1c1-3-1-6-2-4h-1c0-2-5-1-6%201m104%204l-1%201-1%201h-1l-1-2c-3%200-3%202-1%203%201%201%201%201-1%201-2%201-2%200-2-2l-2-2c-2%200-2%201-2%203v3l1-3%201-2%201%202c0%202-2%204-3%203-1%200-4%204-5%209-1%206-1%208%202%205l1-1c0%201%201%202%203%202%204%200%204%200%205-5l2-5v10h11v-13h-9l-9%201%204-2%206-2%201-2c0-2%200-2%201%200%201%203%203%203%204%200%200-3-3-5-5-3m-201%203v3l-2%201c-4%201-7%206-5%207h3c1-2%201-2%201%200-2%206-6%2013-6%209l-2-1v2l2%201v2c2%202%203%201%206-3s5-4%204%200c0%204%203%204%204%201%201-2%201-2%202-1%200%203%205%204%207%203%204-2%203-9-1-9l-3-3-1-2c-4%200-6-4-4-6l1-3c-1-2-5-2-6-1m107%203c-1%201%201%202%208%202%206%200%207%200%207%202s-3%203-6%200h-4l-4%202c-4%200-6%205-4%208h3c2-2%2013-2%2014%200%201%201%200%203-1%202-1%200-2%201-2%203s0%203%202%203l2-1%201-2%201%201%202%202c2%200%202%200%202-2l-1-2-1-1c-2%200-2-2-1-4l1-2c-1-2-5-3-5-1h-13c-3-3%200-4%204-1h4l5-2c3%200%203%200%203-3v-3l-8-1h-9m-77%201c-3%202-3%203%201%204l3-1%2027-1h27c1-1-7-1-26-1l-28-1h-4'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e","aspectRatio":1.3227513227513228,"src":"/static/6ff9fe434382942e85ce0e87a3a4ed21/31987/preview.png","srcSet":"/static/6ff9fe434382942e85ce0e87a3a4ed21/e1953/preview.png 250w,\n/static/6ff9fe434382942e85ce0e87a3a4ed21/46604/preview.png 500w,\n/static/6ff9fe434382942e85ce0e87a3a4ed21/31987/preview.png 1000w,\n/static/6ff9fe434382942e85ce0e87a3a4ed21/0dadc/preview.png 1500w,\n/static/6ff9fe434382942e85ce0e87a3a4ed21/5c082/preview.png 1584w","sizes":"(max-width: 1000px) 100vw, 1000px"}}},"title":"Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation","date":"","tags":["World Model","Benchmark","2025"],"path":"research/2025/wm-bench","excerpt":"Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 517 controlled experiments on 11 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world model abilities. For instance, all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding---e.g., they tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.","links":[{"name":"paper","url":"https://openreview.net/pdf/fad1cdbf1a8687d3e2a1924573c79977176f6b06.pdf"}],"commit":0,"type":"research"},"fileAbsolutePath":"/Users/husky/huskydoge.github.io/example/content/research/2025/wm-bench/index.md","fields":{"slug":{"html":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation\",\n  \"tags\": [\"World Model\", \"Benchmark\", \"2025\"],\n  \"venue\": \"ICLR 2025 Workshop World Models / ACL 2025 Findings\",\n  \"authors\": [{\n    \"name\": \"Qiyue Gao\"\n  }, {\n    \"name\": \"Xinyu Pi\"\n  }, {\n    \"name\": \"Kevin Liu\"\n  }, {\n    \"name\": \"Junrong Chen\"\n  }, {\n    \"name\": \"Ruolan Yang\"\n  }, {\n    \"name\": \"Xinqi Huang\"\n  }, {\n    \"name\": \"Xinyu Fang\"\n  }, {\n    \"name\": \"Lu Sun\"\n  }, {\n    \"name\": \"Gautham Kishore\"\n  }, {\n    \"name\": \"Bo Ai\"\n  }, {\n    \"name\": \"Stone Tao\"\n  }, {\n    \"name\": \"Mengyang Liu\"\n  }, {\n    \"name\": \"Jiaxi Yang\"\n  }, {\n    \"name\": \"Chao-Jung Lai\"\n  }, {\n    \"name\": \"Chuanyang Jin\"\n  }, {\n    \"name\": \"Jiannan Xiang\"\n  }, {\n    \"name\": \"*Benhao Huang*\"\n  }, {\n    \"name\": \"Zeming Chen\"\n  }, {\n    \"name\": \"David Danks\"\n  }, {\n    \"name\": \"Hao Su\"\n  }],\n  \"path\": \"research/2025/wm-bench\",\n  \"excerpt\": \"Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 517 controlled experiments on 11 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world model abilities. For instance, all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding---e.g., they tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.\",\n  \"selected\": false,\n  \"cover\": \"./preview.png\",\n  \"links\": [{\n    \"name\": \"paper\",\n    \"url\": \"https://openreview.net/pdf/fad1cdbf1a8687d3e2a1924573c79977176f6b06.pdf\"\n  }],\n  \"priority\": 2\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"text\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-text\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }))));\n}\n;\nMDXContent.isMDXComponent = true;","htmlEncrypted":"","nonce":""}}}},"pageContext":{"fileAbsolutePath":"/Users/husky/huskydoge.github.io/example/content/research/2025/wm-bench/index.md","postPath":"research/2025/wm-bench","translations":[{"hreflang":"en","path":"/research/2025/wm-bench"}]}},
    "staticQueryHashes": ["1552981879","2213578703","4097791827"]}